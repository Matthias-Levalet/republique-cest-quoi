{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refacto V3 :\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "ns = {\"ns\": \"http://schemas.assemblee-nationale.fr/referentiel\"}\n",
    "\n",
    "\n",
    "def extraire_donnees_v3(fichier_xml: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        tree = ET.parse(fichier_xml)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur de parsing {fichier_xml} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def get_text(path):\n",
    "        elem = root.find(path, ns)\n",
    "        return elem.text.strip() if elem is not None and elem.text else \"\"\n",
    "\n",
    "    # M√©tadonn√©es\n",
    "    metadata = {\n",
    "        \"UID\": get_text(\".//ns:uid\"),\n",
    "        \"SeanceRef\": get_text(\".//ns:seanceRef\"),\n",
    "        \"SessionRef\": get_text(\".//ns:sessionRef\"),\n",
    "        \"DateSeance\": get_text(\".//ns:dateSeance\"),\n",
    "        \"DateSeanceJour\": get_text(\".//ns:dateSeanceJour\"),\n",
    "        \"NumSeanceJour\": get_text(\".//ns:numSeanceJour\"),\n",
    "        \"NumSeance\": get_text(\".//ns:numSeance\"),\n",
    "        \"TypeAssemblee\": get_text(\".//ns:typeAssemblee\"),\n",
    "        \"Legislature\": get_text(\".//ns:legislature\"),\n",
    "        \"Session\": get_text(\".//ns:session\"),\n",
    "        \"NomFichierJO\": get_text(\".//ns:nomFichierJo\"),\n",
    "        \"President\": get_text(\".//ns:presidentSeance\"),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    titre_general = \"\"\n",
    "    sous_titre = \"\"\n",
    "\n",
    "    contenu = root.find(\"ns:contenu\", ns)\n",
    "    if contenu is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for elem in contenu.iter():\n",
    "        tag = elem.tag.split(\"}\")[-1]\n",
    "        code_grammaire = elem.attrib.get(\"code_grammaire\", \"\")\n",
    "        code_style = elem.attrib.get(\"code_style\", \"\")\n",
    "\n",
    "        if code_grammaire == \"TITRE_TEXTE_DISCUSSION\" and code_style == \"Titre\":\n",
    "            texte = elem.find(\"ns:texte\", ns)\n",
    "            titre_general = (\n",
    "                texte.text.strip() if texte is not None and texte.text else \"\"\n",
    "            )\n",
    "\n",
    "        elif (\n",
    "            code_grammaire == \"SOUS_TITRE_TEXTE_DISCUSSION\"\n",
    "            and code_style == \"Sous-tit_info\"\n",
    "        ):\n",
    "            texte = elem.find(\"ns:texte\", ns)\n",
    "            sous_titre = texte.text.strip() if texte is not None and texte.text else \"\"\n",
    "\n",
    "        elif tag == \"paragraphe\":\n",
    "            texte_elem = elem.find(\"ns:texte\", ns)\n",
    "            if texte_elem is None:\n",
    "                continue\n",
    "\n",
    "            texte = \"\".join(texte_elem.itertext()).strip()\n",
    "            stime = texte_elem.attrib.get(\"stime\", \"\")\n",
    "\n",
    "            orateur_elem = elem.find(\".//ns:orateur\", ns)\n",
    "            nom, qualite, id_orateur = \"\", \"\", \"\"\n",
    "            if orateur_elem is not None:\n",
    "                nom = orateur_elem.find(\"ns:nom\", ns)\n",
    "                qualite = orateur_elem.find(\"ns:qualite\", ns)\n",
    "                id_elem = orateur_elem.find(\"ns:id\", ns)\n",
    "\n",
    "                nom = nom.text.strip() if nom is not None and nom.text else \"\"\n",
    "                qualite = (\n",
    "                    qualite.text.strip() if qualite is not None and qualite.text else \"\"\n",
    "                )\n",
    "                id_orateur = (\n",
    "                    id_elem.text.strip() if id_elem is not None and id_elem.text else \"\"\n",
    "                )\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    **metadata,\n",
    "                    \"Titre_general\": titre_general,\n",
    "                    \"Sous_titre\": sous_titre,\n",
    "                    \"ID_paragraphe\": elem.attrib.get(\"id_syceron\", \"\"),\n",
    "                    \"Ordre_seance\": elem.attrib.get(\"ordre_absolu_seance\", \"\"),\n",
    "                    \"Code_grammaire\": code_grammaire,\n",
    "                    \"Code_style\": code_style,\n",
    "                    \"Code_parole\": elem.attrib.get(\"code_parole\", \"\"),\n",
    "                    \"Role_debat\": elem.attrib.get(\"roledebat\", \"\"),\n",
    "                    \"Nom_orateur\": nom,\n",
    "                    \"Qualite_orateur\": qualite,\n",
    "                    \"ID_orateur\": id_orateur,\n",
    "                    \"stime\": stime,\n",
    "                    \"Texte\": texte,\n",
    "                    \"Fichier_source\": os.path.basename(fichier_xml),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def traiter_dossier_v3(dossier_path: str, pattern=\"*.xml\", output_csv=\"export_v3.csv\"):\n",
    "    fichiers = glob.glob(os.path.join(dossier_path, pattern))\n",
    "    if not fichiers:\n",
    "        print(\"‚ùå Aucun fichier XML trouv√©.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîç Traitement de {len(fichiers)} fichiers XML dans le dossier...\")\n",
    "\n",
    "    dfs = []\n",
    "    for fichier in fichiers:\n",
    "        print(f\"‚û°Ô∏è  {os.path.basename(fichier)}\")\n",
    "        df = extraire_donnees_v3(fichier)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if dfs:\n",
    "        df_concat = pd.concat(dfs, ignore_index=True)\n",
    "        df_concat.to_csv(\n",
    "            output_csv, index=False, quoting=csv.QUOTE_ALL, encoding=\"utf-8\"\n",
    "        )\n",
    "        print(f\"\\n‚úÖ Export CSV : {output_csv} ({df_concat.shape[0]} lignes)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun paragraphe extrait.\")\n",
    "\n",
    "\n",
    "# traiter_dossier_v3(\"../data/16-xml/compteRendu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4\n",
    "# Refactorisation claude\n",
    "# Plus d'informations (contexte des points)\n",
    "# Tra√ßabilit√© (source d'extraction)\n",
    "\n",
    "# Essai refactorisation claude\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def extraire_donnees_assemblee(fichier_xml: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrait les donn√©es d'un fichier XML de compte rendu de l'Assembl√©e nationale\n",
    "    Version hybride : combine contexte global ET informations des points ODJ.\n",
    "    \"\"\"\n",
    "\n",
    "    # Charger le XML\n",
    "    tree = ET.parse(fichier_xml)\n",
    "    root = tree.getroot()\n",
    "    ns = {\"ns\": \"http://schemas.assemblee-nationale.fr/referentiel\"}\n",
    "\n",
    "    def get_text(path):\n",
    "        \"\"\"Fonction helper pour extraire le texte de mani√®re s√ªre\"\"\"\n",
    "        elem = root.find(path, ns)\n",
    "        return elem.text.strip() if elem is not None and elem.text else \"\"\n",
    "\n",
    "    # M√©tadonn√©es g√©n√©rales\n",
    "    metadata = {\n",
    "        \"UID\": get_text(\".//ns:uid\"),\n",
    "        \"SeanceRef\": get_text(\".//ns:seanceRef\"),\n",
    "        \"SessionRef\": get_text(\".//ns:sessionRef\"),\n",
    "        \"DateSeance\": get_text(\".//ns:dateSeance\"),\n",
    "        \"DateSeanceJour\": get_text(\".//ns:dateSeanceJour\"),\n",
    "        \"NumSeanceJour\": get_text(\".//ns:numSeanceJour\"),\n",
    "        \"NumSeance\": get_text(\".//ns:numSeance\"),\n",
    "        \"TypeAssemblee\": get_text(\".//ns:typeAssemblee\"),\n",
    "        \"Legislature\": get_text(\".//ns:legislature\"),\n",
    "        \"Session\": get_text(\".//ns:session\"),\n",
    "        \"NomFichierJO\": get_text(\".//ns:nomFichierJo\"),\n",
    "        \"President\": get_text(\".//ns:presidentSeance\"),\n",
    "    }\n",
    "\n",
    "    # √âTAPE 1 : Cr√©er le mapping paragraphe -> point (contexte local)\n",
    "    paragraphe_to_point = _creer_mapping_points(root, ns)\n",
    "\n",
    "    # √âTAPE 2 : Parcourir le contenu et extraire tous les paragraphes\n",
    "    contenu = root.find(\"ns:contenu\", ns)\n",
    "    if contenu is None:\n",
    "        print(f\"‚ö†Ô∏è Pas de contenu trouv√© dans {fichier_xml}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    contexte_global = _initialiser_contexte()\n",
    "\n",
    "    for elem in contenu.iter():\n",
    "        tag = elem.tag.split(\"}\")[-1]\n",
    "        code_grammaire = elem.attrib.get(\"code_grammaire\", \"\")\n",
    "        code_style = elem.attrib.get(\"code_style\", \"\")\n",
    "\n",
    "        # Mettre √† jour le contexte global\n",
    "        _mettre_a_jour_contexte(elem, contexte_global, ns)\n",
    "\n",
    "        # Traiter les paragraphes\n",
    "        if tag == \"paragraphe\":\n",
    "            row = _extraire_paragraphe(\n",
    "                elem, metadata, contexte_global, paragraphe_to_point, ns\n",
    "            )\n",
    "            if row:\n",
    "                rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _creer_mapping_points(root, ns):\n",
    "    \"\"\"\n",
    "    Cr√©e un mapping entre les paragraphes et leurs points parents\n",
    "    pour r√©cup√©rer le contexte local (sujet du point, valeur ODJ).\n",
    "    \"\"\"\n",
    "    paragraphe_to_point = {}\n",
    "\n",
    "    for point in root.findall(\".//ns:point\", ns):\n",
    "        # Informations du point\n",
    "        sujet_elem = point.find(\"ns:texte\", ns)\n",
    "        sujet_texte = (\n",
    "            sujet_elem.text.strip()\n",
    "            if sujet_elem is not None and sujet_elem.text\n",
    "            else \"\"\n",
    "        )\n",
    "        valeur_ptsodj = point.attrib.get(\"valeur_ptsodj\", \"\")\n",
    "\n",
    "        # R√©cup√©rer d'autres m√©tadonn√©es du point si disponibles\n",
    "        point_id = point.attrib.get(\"id\", \"\")\n",
    "        point_type = point.attrib.get(\"type\", \"\")\n",
    "\n",
    "        point_info = {\n",
    "            \"sujet\": sujet_texte,\n",
    "            \"valeur_odj\": valeur_ptsodj,\n",
    "            \"point_id\": point_id,\n",
    "            \"point_type\": point_type,\n",
    "        }\n",
    "\n",
    "        # Associer tous les paragraphes de ce point (recherche r√©cursive)\n",
    "        for para in point.findall(\".//ns:paragraphe\", ns):\n",
    "            para_id = para.attrib.get(\"id_syceron\", \"\")\n",
    "            if para_id:  # Seulement si on a un ID valide\n",
    "                paragraphe_to_point[para_id] = point_info\n",
    "            else:\n",
    "                # Fallback : utiliser l'objet Python comme cl√©\n",
    "                paragraphe_to_point[id(para)] = point_info\n",
    "\n",
    "    return paragraphe_to_point\n",
    "\n",
    "\n",
    "def _initialiser_contexte():\n",
    "    \"\"\"Initialise la structure de contexte global\"\"\"\n",
    "    return {\n",
    "        \"titre_general\": \"\",\n",
    "        \"sous_titre\": \"\",\n",
    "        \"contexte_stack\": [],\n",
    "        \"section_courante\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def _mettre_a_jour_contexte(elem, contexte, ns):\n",
    "    \"\"\"\n",
    "    Met √† jour le contexte global bas√© sur l'√©l√©ment courant.\n",
    "    G√®re diff√©rents niveaux hi√©rarchiques de titres.\n",
    "    \"\"\"\n",
    "    code_grammaire = elem.attrib.get(\"code_grammaire\", \"\")\n",
    "    code_style = elem.attrib.get(\"code_style\", \"\")\n",
    "\n",
    "    texte_elem = elem.find(\"ns:texte\", ns)\n",
    "    texte = (\n",
    "        texte_elem.text.strip() if texte_elem is not None and texte_elem.text else \"\"\n",
    "    )\n",
    "\n",
    "    if not texte:\n",
    "        return\n",
    "\n",
    "    # Titre principal\n",
    "    if code_grammaire == \"TITRE_TEXTE_DISCUSSION\" and code_style == \"Titre\":\n",
    "        contexte[\"titre_general\"] = texte\n",
    "        contexte[\"contexte_stack\"] = [texte]\n",
    "\n",
    "    # Sous-titre\n",
    "    elif (\n",
    "        code_grammaire == \"SOUS_TITRE_TEXTE_DISCUSSION\"\n",
    "        and code_style == \"Sous-tit_info\"\n",
    "    ):\n",
    "        contexte[\"sous_titre\"] = texte\n",
    "        if len(contexte[\"contexte_stack\"]) == 1:\n",
    "            contexte[\"contexte_stack\"].append(texte)\n",
    "        else:\n",
    "            contexte[\"contexte_stack\"] = [contexte[\"titre_general\"], texte]\n",
    "\n",
    "    # Autres types de sections\n",
    "    elif code_grammaire in [\"TITRE_RUBRIQUE\", \"SOUS_TITRE_RUBRIQUE\"]:\n",
    "        if \"SOUS\" in code_grammaire:\n",
    "            if len(contexte[\"contexte_stack\"]) >= 2:\n",
    "                contexte[\"contexte_stack\"][1] = texte\n",
    "            else:\n",
    "                contexte[\"contexte_stack\"].append(texte)\n",
    "        else:\n",
    "            contexte[\"contexte_stack\"] = [texte]\n",
    "\n",
    "    # Section courante pour d'autres cas\n",
    "    elif code_style in [\"Titre\", \"Sous-tit\", \"Sous-tit_info\"]:\n",
    "        contexte[\"section_courante\"] = texte\n",
    "\n",
    "\n",
    "def _extraire_paragraphe(elem, metadata, contexte_global, paragraphe_to_point, ns):\n",
    "    \"\"\"\n",
    "    Extrait toutes les informations d'un paragraphe.\n",
    "    Combine contexte global et contexte local (point).\n",
    "    \"\"\"\n",
    "    texte_elem = elem.find(\"ns:texte\", ns)\n",
    "    if texte_elem is None:\n",
    "        return None\n",
    "\n",
    "    # Extraction du texte avec gestion des balises internes\n",
    "    texte = \"\".join(texte_elem.itertext()).strip()\n",
    "    if not texte:  # Skip les paragraphes vides\n",
    "        return None\n",
    "\n",
    "    stime = texte_elem.attrib.get(\"stime\", \"\")\n",
    "\n",
    "    # Informations orateur\n",
    "    nom, qualite, id_orateur = _extraire_orateur(elem, ns)\n",
    "\n",
    "    # R√©cup√©rer le contexte du point parent (si disponible)\n",
    "    para_id = elem.attrib.get(\"id_syceron\", \"\")\n",
    "    point_info = paragraphe_to_point.get(para_id) or paragraphe_to_point.get(\n",
    "        id(elem), {}\n",
    "    )\n",
    "\n",
    "    # D√©terminer la source d'extraction\n",
    "    source_extraction = \"point\" if point_info else \"global\"\n",
    "\n",
    "    # Construire le contexte hi√©rarchique complet\n",
    "    contexte_hierarchique = \" > \".join(filter(None, contexte_global[\"contexte_stack\"]))\n",
    "\n",
    "    return {\n",
    "        **metadata,\n",
    "        # Contexte global\n",
    "        \"Titre_general\": contexte_global[\"titre_general\"],\n",
    "        \"Sous_titre\": contexte_global[\"sous_titre\"],\n",
    "        \"Contexte_hierarchique\": contexte_hierarchique,\n",
    "        \"Section_courante\": contexte_global[\"section_courante\"],\n",
    "        # Contexte local du point\n",
    "        \"Sujet_point\": point_info.get(\"sujet\", \"\"),\n",
    "        \"Valeur_ODJ\": point_info.get(\"valeur_odj\", \"\"),\n",
    "        \"Point_ID\": point_info.get(\"point_id\", \"\"),\n",
    "        \"Point_type\": point_info.get(\"point_type\", \"\"),\n",
    "        # Informations du paragraphe\n",
    "        \"ID_paragraphe\": para_id,\n",
    "        \"Ordre_seance\": elem.attrib.get(\"ordre_absolu_seance\", \"\"),\n",
    "        \"Code_grammaire\": elem.attrib.get(\"code_grammaire\", \"\"),\n",
    "        \"Code_style\": elem.attrib.get(\"code_style\", \"\"),\n",
    "        \"Code_parole\": elem.attrib.get(\"code_parole\", \"\"),\n",
    "        \"Role_debat\": elem.attrib.get(\"roledebat\", \"\"),\n",
    "        # Informations de l'orateur\n",
    "        \"Nom_orateur\": nom,\n",
    "        \"Qualite_orateur\": qualite,\n",
    "        \"ID_orateur\": id_orateur,\n",
    "        # Contenu\n",
    "        \"stime\": stime,\n",
    "        \"Texte\": texte,\n",
    "        # M√©tadonn√©es d'extraction\n",
    "        \"Source_extraction\": source_extraction,\n",
    "        # nope \"Fichier_source\": os.path.basename(elem.getroottree().getroot().attrib.get(\"fichier\", \"\"))\n",
    "    }\n",
    "\n",
    "\n",
    "def _extraire_orateur(elem, ns):\n",
    "    \"\"\"Extrait les informations de l'orateur de mani√®re robuste\"\"\"\n",
    "    orateur_elem = elem.find(\".//ns:orateur\", ns)\n",
    "    nom, qualite, id_orateur = \"\", \"\", \"\"\n",
    "\n",
    "    if orateur_elem is not None:\n",
    "        nom_elem = orateur_elem.find(\"ns:nom\", ns)\n",
    "        nom = nom_elem.text.strip() if nom_elem is not None and nom_elem.text else \"\"\n",
    "\n",
    "        qualite_elem = orateur_elem.find(\"ns:qualite\", ns)\n",
    "        qualite = (\n",
    "            qualite_elem.text.strip()\n",
    "            if qualite_elem is not None and qualite_elem.text\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        id_elem = orateur_elem.find(\"ns:id\", ns)\n",
    "        id_orateur = (\n",
    "            id_elem.text.strip() if id_elem is not None and id_elem.text else \"\"\n",
    "        )\n",
    "\n",
    "    return nom, qualite, id_orateur\n",
    "\n",
    "\n",
    "def traiter_dossier_xml(dossier_path: str, pattern: str = \"*.xml\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Traite tous les fichiers XML d'un dossier et retourne un DataFrame consolid√©.\n",
    "    \"\"\"\n",
    "    fichiers_xml = glob.glob(os.path.join(dossier_path, pattern))\n",
    "\n",
    "    if not fichiers_xml:\n",
    "        print(f\"‚ùå Aucun fichier XML trouv√© dans {dossier_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tous_dataframes = []\n",
    "    stats = {\"success\": 0, \"errors\": 0, \"total_rows\": 0}\n",
    "\n",
    "    print(f\"üîÑ Traitement de {len(fichiers_xml)} fichiers XML...\")\n",
    "\n",
    "    for fichier in fichiers_xml:\n",
    "        nom_fichier = os.path.basename(fichier)\n",
    "        print(f\"   üìÑ {nom_fichier}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            df = extraire_donnees_assemblee(fichier)\n",
    "            if not df.empty:\n",
    "                tous_dataframes.append(df)\n",
    "                stats[\"success\"] += 1\n",
    "                stats[\"total_rows\"] += len(df)\n",
    "                print(f\"‚úÖ {len(df)} lignes\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Vide\")\n",
    "\n",
    "        except Exception as e:\n",
    "            stats[\"errors\"] += 1\n",
    "            print(f\"‚ùå Erreur: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nüìä Statistiques:\")\n",
    "    print(f\"   ‚úÖ Fichiers trait√©s avec succ√®s: {stats['success']}\")\n",
    "    print(f\"   ‚ùå Erreurs: {stats['errors']}\")\n",
    "    print(f\"   üìù Total de lignes extraites: {stats['total_rows']}\")\n",
    "\n",
    "    if tous_dataframes:\n",
    "        df_final = pd.concat(tous_dataframes, ignore_index=True)\n",
    "\n",
    "        # Statistiques sur la r√©partition des sources\n",
    "        if \"Source_extraction\" in df_final.columns:\n",
    "            source_stats = df_final[\"Source_extraction\"].value_counts()\n",
    "            print(f\"\\nüéØ R√©partition des sources:\")\n",
    "            for source, count in source_stats.items():\n",
    "                print(\n",
    "                    f\"   {source}: {count} paragraphes ({count / len(df_final) * 100:.1f}%)\"\n",
    "                )\n",
    "\n",
    "        return df_final\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def analyser_extraction(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyse les r√©sultats de l'extraction pour identifier les diff√©rences.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå DataFrame vide\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüìä ANALYSE DE L'EXTRACTION\")\n",
    "    print(f\"Total de paragraphes: {len(df)}\")\n",
    "\n",
    "    # R√©partition par source\n",
    "    if \"Source_extraction\" in df.columns:\n",
    "        print(f\"\\nüîç R√©partition par source:\")\n",
    "        source_counts = df[\"Source_extraction\"].value_counts()\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"   {source}: {count} ({count / len(df) * 100:.1f}%)\")\n",
    "\n",
    "    # Paragraphes avec vs sans contexte de point\n",
    "    if \"Sujet_point\" in df.columns:\n",
    "        avec_point = df[df[\"Sujet_point\"] != \"\"]\n",
    "        sans_point = df[df[\"Sujet_point\"] == \"\"]\n",
    "        print(f\"\\nüìù Contexte des points:\")\n",
    "        print(f\"   Avec sujet de point: {len(avec_point)}\")\n",
    "        print(f\"   Sans sujet de point: {len(sans_point)}\")\n",
    "\n",
    "    # Types de codes grammaire les plus fr√©quents\n",
    "    if \"Code_grammaire\" in df.columns:\n",
    "        print(f\"\\nüè∑Ô∏è Top 5 des codes grammaire:\")\n",
    "        top_codes = df[\"Code_grammaire\"].value_counts().head()\n",
    "        for code, count in top_codes.items():\n",
    "            print(f\"   {code or '(vide)'}: {count}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# # Pour un seul fichier\n",
    "# print(\"üöÄ Test sur un fichier unique...\")\n",
    "# df = extraire_donnees_assemblee(\"CRSANR5L16S2024O1N220.xml\")\n",
    "\n",
    "# # Analyse des r√©sultats\n",
    "# analyser_extraction(df)\n",
    "\n",
    "# # Sauvegarde\n",
    "# if not df.empty:\n",
    "#     df.to_csv(\"assemblee_debat_hybride_ameliore.csv\",\n",
    "#               index=False,\n",
    "#               quoting=csv.QUOTE_ALL,\n",
    "#               encoding='utf-8')\n",
    "#     print(f\"\\n‚úÖ Fichier CSV g√©n√©r√© : assemblee_debat_hybride_ameliore.csv\")\n",
    "\n",
    "# # Pour un dossier entier (d√©commenter si besoin)\n",
    "# df_complet = traiter_dossier_xml(\"../data/16-xml/compteRendu/\")\n",
    "# analyser_extraction(df_complet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e017e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = extraire_donnees_assemblee(\"CRSANR5L16S2024O1N220.xml\")\n",
    "df_temp.to_csv(\"V4.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
