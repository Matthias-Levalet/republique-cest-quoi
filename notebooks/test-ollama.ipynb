{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f284a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug incompatibilit√© dans myenv_clone, ok dans nlp_env\n",
    "# faire du nettoyage dans les envs un jour ‚Ä¶\n",
    "import os\n",
    "from ollama import Client\n",
    "\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\")\n",
    "client = Client(host=OLLAMA_HOST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5bf4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/df_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf29366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2' created_at='2025-06-26T14:35:44.364221377Z' done=True done_reason='stop' total_duration=1291411302 load_duration=14430832 prompt_eval_count=31 prompt_eval_duration=1834207 eval_count=283 eval_duration=1274251929 message=Message(role='assistant', content=\"The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\\n\\nHere's what happens:\\n\\n1. Sunlight enters Earth's atmosphere and is made up of different wavelengths (colors) of light.\\n2. The shorter (blue) wavelengths are scattered more than the longer (red) wavelengths by the tiny molecules of gases such as nitrogen and oxygen in the atmosphere.\\n3. This scattering effect is more pronounced for blue light because it has a smaller wavelength, which means it's more easily deflected by the tiny molecules.\\n4. As a result, when sunlight enters our eyes, we see mostly the scattered blue light, making the sky appear blue.\\n\\nIt's worth noting that the color of the sky can vary depending on the time of day, atmospheric conditions, and other factors. For example:\\n\\n* During sunrise and sunset, the sky often takes on hues of red, orange, and pink due to the scattering of longer wavelengths.\\n* In areas with high levels of pollution or dust, the sky may appear more gray or hazy.\\n* At night, the sky can appear darker because our eyes are adapted to see in low light conditions.\\n\\nHowever, under clear conditions, the blue color of the sky is a result of Rayleigh scattering and remains one of the most iconic and striking features of our natural world.\", images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# # test\n",
    "# response = client.chat(\n",
    "#     model=\"llama3.2\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Why is the sky blue?\",\n",
    "#         },\n",
    "#     ],\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e888d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to check prompt struct to be sure of what is going to ollama\n",
    "def get_prompt(text):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Tu es un expert en sciences politique. \"\n",
    "                \"Classifie chaque intervention comme relevant du th√®me 'REPUBLIQUE' ou 'AUTRE'\"\n",
    "                \"R√©ponds uniquement par un de ces deux mots\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f'Classe cette intervention :\\n\"{text}\"\\nLabel:'},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ed61b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_predictions(prompt_generator, texts, model):\n",
    "    \"\"\"\n",
    "    Prediction avec Ollama : prompt_generator, liste de textes, mod√®le.\n",
    "    Affiche la progression (tqdm).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(texts)\n",
    "\n",
    "    with tqdm(\n",
    "        total=total,\n",
    "        desc=\"Progress\",\n",
    "        unit=\"item\",\n",
    "        bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt} [{percentage:3.0f}%]\",\n",
    "    ) as pbar:\n",
    "        for i, text in texts.items():\n",
    "            try:\n",
    "                messages = prompt_generator(text)\n",
    "                response = client.chat(model=model, messages=messages)\n",
    "                content = response.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                results.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur ligne {i} : {e}\")\n",
    "                results.append(None)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # S'inspirer de √ßa si besoin ?\n",
    "# # The structure is a bit different\n",
    "# def build_prompt(text):\n",
    "#     system_prompt = (\n",
    "#         \"You are a helpful and accurate news headline classifier. \"\n",
    "#         \"Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. \"\n",
    "#         \"Only respond with exactly one of those two labels.\"\n",
    "#     )\n",
    "#     user_prompt = f\"Classify this headline:\\n\\\"{text}\\\"\\nLabel:\"\n",
    "#     return f\"<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n[INST] {user_prompt} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# si abesoin ajout pause, un truc du genre ?\n",
    "\n",
    "# # test avec un extrait\n",
    "# texts = df[\"Texte\"][:100]  # ou .iloc[:blabla]\n",
    "\n",
    "start = time.time()\n",
    "# Appel\n",
    "r_ollama = do_predictions(get_prompt, df[\"Texte\"], model=\"llama3.1:70b\")\n",
    "end = time.time()\n",
    "print(f\"Temps d'ex√©cution : {end - start:.2f} secondes\")\n",
    "\n",
    "# # Ajout dans le DataFrame\n",
    "# df.loc[texts.index, \"prediction_simple\"] = r_ollama\n",
    "\n",
    "# guess it kinda work ?\n",
    "df.loc[df[\"Texte\"].index, \"prediction_simple\"] = r_ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8341764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_pred = df[[\"ID_paragraphe\", \"prediction_simple\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a644215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_pred.to_csv(\"../data/intermediate/df_full_pred_simple.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d84899",
   "metadata": {},
   "source": [
    "## longer prompt ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to check prompt struct to be sure of what is going to ollama\n",
    "\n",
    "\n",
    "def get_prompt_long(text):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Dans le cadre d‚Äôun article acad√©mique en science politique, tu dois distinguer les interventions relatives √† la 'R√©publique' des 'autres' \"\n",
    "                \"Il s‚Äôagit d‚Äôun corpus d‚Äôinterventions de d√©put√©s √† l‚Äôassembl√©e nationale en France lors de la 16e l√©gislature.\"\n",
    "                \" La ¬´ R√©publique ¬ª renvoie ici √† sa dimension id√©elle ou id√©ologique. Est donc exclut les r√©f√©rences √† des pays, au nom d‚Äôun parti politique, d‚Äôun journaliste ou d‚Äôune institution, ou √† la fonction de pr√©sident de la R√©publique. Est inclut en plus du champ lexical de la ¬´ R√©publique ¬ª, √† la devise ¬´ libert√©, √©galit√©, fraternit√© ¬ª et aux principes de ¬´ la√Øcit√© ¬ª, ¬´ universalisme ¬ª et ¬´ indivisibilit√© ¬ª. \"\n",
    "                \"Classifie chaque intervention comme 'REPUBLIQUE' ou 'AUTRE'\"\n",
    "                \"R√©ponds uniquement par un de ces deux mots.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f'Classe cette intervention :\\n\"{text}\"\\nLabel:'},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8ab6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 100/100 [100%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'ex√©cution : 78.47 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# test avec un extrait\n",
    "texts = df[\"Texte\"][:100]  # ou .iloc[:10]\n",
    "\n",
    "start = time.time()\n",
    "# Appel\n",
    "r_ollama = do_predictions(get_prompt_long, texts, model=\"llama3.1:70b\")\n",
    "end = time.time()\n",
    "print(f\"Temps d'ex√©cution : {end - start:.2f} secondes\")\n",
    "\n",
    "# Ajout dans le DataFrame\n",
    "df.loc[texts.index, \"prediction\"] = r_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88af42e",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108545e0",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85769158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_paragraphe</th>\n",
       "      <th>prediction_long</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3043935</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3044004</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3044275</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>AUTRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3044207</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3044651</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_paragraphe prediction_long  prediction\n",
       "0        3043935           AUTRE  REPUBLIQUE\n",
       "1        3044004           AUTRE  REPUBLIQUE\n",
       "2        3044275           AUTRE       AUTRE\n",
       "3        3044207           AUTRE  REPUBLIQUE\n",
       "4        3044651           AUTRE  REPUBLIQUE"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "df_score = pd.read_csv(\"../data/intermediate/df_merged_pred.csv\")\n",
    "df_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       AUTRE      0.625     0.172     0.270        29\n",
      "      AUTRE.      0.000     0.000     0.000         0\n",
      "  REPUBLIQUE      0.739     0.915     0.818        71\n",
      "  R√©publique      0.000     0.000     0.000         0\n",
      " R√©publique.      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.700       100\n",
      "   macro avg      0.273     0.218     0.218       100\n",
      "weighted avg      0.706     0.700     0.659       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# pour le principe, puisque ici pas de gold standard, juste les mod√®les entre eux\n",
    "df_score = df_score[0:100]\n",
    "print(classification_report(df_score[\"prediction_long\"], df_score[\"prediction\"], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cacea899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21757606663267043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_score[\"prediction_long\"], df_score[\"prediction\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e555ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>AUTRE</th>\n",
       "      <th>AUTRE.</th>\n",
       "      <th>REPUBLIQUE</th>\n",
       "      <th>R√©publique</th>\n",
       "      <th>R√©publique.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_long</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUTRE</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REPUBLIQUE</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction       AUTRE  AUTRE.  REPUBLIQUE  R√©publique  R√©publique.\n",
       "prediction_long                                                    \n",
       "AUTRE                5       1          23           0            0\n",
       "REPUBLIQUE           3       1          65           1            1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparer les mod√®les\n",
    "pd.crosstab(df_score[\"prediction_long\"], df_score[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67121355",
   "metadata": {},
   "source": [
    "## Few-shot ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54426f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's build the prompt, with some examples\n",
    "# few_shot_examples = [\n",
    "#         (\"Biden signs executive order on student debt relief\", \"POLITICS\"),\n",
    "#         (\"Amazon launches new AI-powered Alexa features\", \"OTHER\"),\n",
    "#         (\"Congress debates military spending bill\", \"POLITICS\"),\n",
    "#         (\"PSG wins much-anticipated Champions League\", \"OTHER\")\n",
    "#         ]\n",
    "\n",
    "# def get_prompt_few_shots(text: str, examples: list[tuple] = few_shot_examples):\n",
    "#   examples = \"\\n\".join([f\"Classify this headline:\\n{headline}\\nLabel: {label}\\n\\n\" for headline, label in examples])\n",
    "#   return [{\"role\":\"system\",\n",
    "#            \"content\":\"You are a strict news classifier. You must respond with one word only ‚Äî either 'POLITICS' or 'OTHER'.\" +\n",
    "#            \"\\n Do not explain. Do not output anything else.\"\n",
    "\n",
    "#            },\n",
    "#            {\"role\":\"user\",\"content\": f\"{examples}\\nClassify this headline:\\n\\\"{text}\\\"\\nLabel:\"}\n",
    "#   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d771928",
   "metadata": {},
   "source": [
    "## Refacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspiration GPT pour avoir des checkpoints :\n",
    "# results = []\n",
    "# indexes = []\n",
    "# batch_size = 20\n",
    "\n",
    "# for i in range(0, len(df), batch_size):\n",
    "#     batch = df[\"Texte\"].iloc[i:i+batch_size]\n",
    "#     try:\n",
    "#         r_batch, _ = do_predictions(get_prompt, batch, model=\"llama3.1:70b\")\n",
    "#         results.extend(r_batch)\n",
    "#         indexes.extend(batch.index)\n",
    "\n",
    "#         # Checkpoint every 100 samples\n",
    "#         if i % 100 == 0:\n",
    "#             df.loc[indexes, \"prediction_simple\"] = results\n",
    "#             df.to_csv(\"checkpoint_predictions.csv\", index=False)\n",
    "#             print(f\"Checkpoint sauvegard√© √† {i}.\")\n",
    "\n",
    "#         time.sleep(1)  # optional pause\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur au batch {i}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# df.loc[indexes, \"prediction_simple\"] = results\n",
    "# df.to_csv(\"final_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf68914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Et m√™me inspiration pour un truc encore plus complet, mais overkill pour l'instant\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# def ensure_dir(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "\n",
    "\n",
    "# def timestamp():\n",
    "#     return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# def do_predictions(prompt_func, texts, model):\n",
    "#     prompt_name = prompt_func.__name__\n",
    "#     results = [prompt_func(text) for text in texts]\n",
    "#     return results, prompt_name\n",
    "\n",
    "\n",
    "# def run_llm_batches(df, text_column, prompt_func, model,\n",
    "#                     column_name=None, batch_size=20, sleep_time=1,\n",
    "#                     checkpoint_every=100, output_dir=\"outputs\"):\n",
    "\n",
    "#     # Setup\n",
    "#     start = time.time()\n",
    "#     all_results = []\n",
    "#     all_indexes = []\n",
    "#     name = prompt_func.__name__ if column_name is None else column_name\n",
    "\n",
    "#     ensure_dir(output_dir)\n",
    "\n",
    "#     print(f\"üöÄ D√©but du traitement avec le prompt '{name}' et le mod√®le '{model}'\")\n",
    "\n",
    "#     # Boucle par batch\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batch = df[text_column].iloc[i:i + batch_size]\n",
    "#         idx = batch.index\n",
    "\n",
    "#         try:\n",
    "#             batch_start = time.time()\n",
    "#             r_batch, _ = do_predictions(prompt_func, batch, model=model)\n",
    "#             all_results.extend(r_batch)\n",
    "#             all_indexes.extend(idx)\n",
    "#             print(f\"‚úÖ Batch {i}-{i + batch_size} trait√© en {time.time() - batch_start:.2f}s\")\n",
    "\n",
    "#             # Sauvegarde p√©riodique\n",
    "#             if (i // batch_size) % (checkpoint_every // batch_size) == 0:\n",
    "#                 df.loc[all_indexes, name] = all_results\n",
    "#                 ckpt_file = os.path.join(output_dir, f\"checkpoint_{name}_{timestamp()}.csv\")\n",
    "#                 df.to_csv(ckpt_file, index=False)\n",
    "#                 print(f\"üíæ Checkpoint enregistr√© : {ckpt_file}\")\n",
    "\n",
    "#             time.sleep(sleep_time)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Erreur au batch {i}-{i + batch_size}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     # R√©sultat final\n",
    "#     df.loc[all_indexes, name] = all_results\n",
    "#     final_file = os.path.join(output_dir, f\"final_{name}_{timestamp()}.csv\")\n",
    "#     df.to_csv(final_file, index=False)\n",
    "\n",
    "#     print(f\"üèÅ Traitement termin√© en {time.time() - start:.2f}s\")\n",
    "#     print(f\"üìù R√©sultat final enregistr√© : {final_file}\")\n",
    "#     return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
