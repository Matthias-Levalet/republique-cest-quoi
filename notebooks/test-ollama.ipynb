{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f284a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug incompatibilité dans myenv_clone, ok dans nlp_env\n",
    "# faire du nettoyage dans les envs un jour …\n",
    "import os\n",
    "from ollama import Client\n",
    "\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\")\n",
    "client = Client(host=OLLAMA_HOST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5bf4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/df_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf29366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.2' created_at='2025-06-26T14:35:44.364221377Z' done=True done_reason='stop' total_duration=1291411302 load_duration=14430832 prompt_eval_count=31 prompt_eval_duration=1834207 eval_count=283 eval_duration=1274251929 message=Message(role='assistant', content=\"The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\\n\\nHere's what happens:\\n\\n1. Sunlight enters Earth's atmosphere and is made up of different wavelengths (colors) of light.\\n2. The shorter (blue) wavelengths are scattered more than the longer (red) wavelengths by the tiny molecules of gases such as nitrogen and oxygen in the atmosphere.\\n3. This scattering effect is more pronounced for blue light because it has a smaller wavelength, which means it's more easily deflected by the tiny molecules.\\n4. As a result, when sunlight enters our eyes, we see mostly the scattered blue light, making the sky appear blue.\\n\\nIt's worth noting that the color of the sky can vary depending on the time of day, atmospheric conditions, and other factors. For example:\\n\\n* During sunrise and sunset, the sky often takes on hues of red, orange, and pink due to the scattering of longer wavelengths.\\n* In areas with high levels of pollution or dust, the sky may appear more gray or hazy.\\n* At night, the sky can appear darker because our eyes are adapted to see in low light conditions.\\n\\nHowever, under clear conditions, the blue color of the sky is a result of Rayleigh scattering and remains one of the most iconic and striking features of our natural world.\", images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# # test\n",
    "# response = client.chat(\n",
    "#     model=\"llama3.2\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Why is the sky blue?\",\n",
    "#         },\n",
    "#     ],\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e888d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to check prompt struct to be sure of what is going to ollama\n",
    "def get_prompt(text):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Tu es un expert en sciences politique. \"\n",
    "                \"Classifie chaque intervention comme relevant du thème 'REPUBLIQUE' ou 'AUTRE'\"\n",
    "                \"Réponds uniquement par un de ces deux mots\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f'Classe cette intervention :\\n\"{text}\"\\nLabel:'},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ed61b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_predictions(prompt_generator, texts, model):\n",
    "    \"\"\"\n",
    "    Prediction avec Ollama : prompt_generator, liste de textes, modèle.\n",
    "    Affiche la progression (tqdm).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(texts)\n",
    "\n",
    "    with tqdm(\n",
    "        total=total,\n",
    "        desc=\"Progress\",\n",
    "        unit=\"item\",\n",
    "        bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt} [{percentage:3.0f}%]\",\n",
    "    ) as pbar:\n",
    "        for i, text in texts.items():\n",
    "            try:\n",
    "                messages = prompt_generator(text)\n",
    "                response = client.chat(model=model, messages=messages)\n",
    "                content = response.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                results.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur ligne {i} : {e}\")\n",
    "                results.append(None)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # S'inspirer de ça si besoin ?\n",
    "# # The structure is a bit different\n",
    "# def build_prompt(text):\n",
    "#     system_prompt = (\n",
    "#         \"You are a helpful and accurate news headline classifier. \"\n",
    "#         \"Your job is to classify news headlines as either 'POLITICS' or 'OTHER'. \"\n",
    "#         \"Only respond with exactly one of those two labels.\"\n",
    "#     )\n",
    "#     user_prompt = f\"Classify this headline:\\n\\\"{text}\\\"\\nLabel:\"\n",
    "#     return f\"<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n[INST] {user_prompt} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# si abesoin ajout pause, un truc du genre ?\n",
    "\n",
    "# # test avec un extrait\n",
    "# texts = df[\"Texte\"][:100]  # ou .iloc[:blabla]\n",
    "\n",
    "start = time.time()\n",
    "# Appel\n",
    "r_ollama = do_predictions(get_prompt, df[\"Texte\"], model=\"llama3.1:70b\")\n",
    "end = time.time()\n",
    "print(f\"Temps d'exécution : {end - start:.2f} secondes\")\n",
    "\n",
    "# # Ajout dans le DataFrame\n",
    "# df.loc[texts.index, \"prediction_simple\"] = r_ollama\n",
    "\n",
    "# guess it kinda work ? puisque à tout pris.\n",
    "# Si slice ici aussi renvoyer le slice.index\n",
    "df.loc[df[\"Texte\"].index, \"prediction_simple\"] = r_ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8341764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_pred = df[[\"ID_paragraphe\", \"prediction_simple\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a644215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_pred.to_csv(\"../data/intermediate/df_full_pred_simple.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d84899",
   "metadata": {},
   "source": [
    "## longer prompt ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to check prompt struct to be sure of what is going to ollama\n",
    "\n",
    "\n",
    "def get_prompt_long(text):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Dans le cadre d’un article académique en science politique, tu dois distinguer les interventions relatives à la 'République' des 'autres' \"\n",
    "                \"Il s’agit d’un corpus d’interventions de députés à l’assemblée nationale en France lors de la 16e législature.\"\n",
    "                \" La « République » renvoie ici à sa dimension idéelle ou idéologique. Est donc exclut les références à des pays, au nom d’un parti politique, d’un journaliste ou d’une institution, ou à la fonction de président de la République. Est inclut en plus du champ lexical de la « République », à la devise « liberté, égalité, fraternité » et aux principes de « laïcité », « universalisme » et « indivisibilité ». \"\n",
    "                \"Classifie chaque intervention comme 'REPUBLIQUE' ou 'AUTRE'\"\n",
    "                \"Réponds uniquement par un de ces deux mots.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f'Classe cette intervention :\\n\"{text}\"\\nLabel:'},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8ab6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████ | 100/100 [100%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution : 78.47 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# test avec un extrait\n",
    "texts = df[\"Texte\"][:100]  # ou .iloc[:10]\n",
    "\n",
    "start = time.time()\n",
    "# Appel\n",
    "r_ollama = do_predictions(get_prompt_long, texts, model=\"llama3.1:70b\")\n",
    "end = time.time()\n",
    "print(f\"Temps d'exécution : {end - start:.2f} secondes\")\n",
    "\n",
    "# Ajout dans le DataFrame\n",
    "df.loc[texts.index, \"prediction\"] = r_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88af42e",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108545e0",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85769158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_paragraphe</th>\n",
       "      <th>prediction_long</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3043935</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3044004</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3044275</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>AUTRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3044207</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3044651</td>\n",
       "      <td>AUTRE</td>\n",
       "      <td>REPUBLIQUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_paragraphe prediction_long  prediction\n",
       "0        3043935           AUTRE  REPUBLIQUE\n",
       "1        3044004           AUTRE  REPUBLIQUE\n",
       "2        3044275           AUTRE       AUTRE\n",
       "3        3044207           AUTRE  REPUBLIQUE\n",
       "4        3044651           AUTRE  REPUBLIQUE"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "df_score = pd.read_csv(\"../data/intermediate/df_merged_pred.csv\")\n",
    "df_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       AUTRE      0.625     0.172     0.270        29\n",
      "      AUTRE.      0.000     0.000     0.000         0\n",
      "  REPUBLIQUE      0.739     0.915     0.818        71\n",
      "  République      0.000     0.000     0.000         0\n",
      " République.      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.700       100\n",
      "   macro avg      0.273     0.218     0.218       100\n",
      "weighted avg      0.706     0.700     0.659       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/leo/anaconda3/envs/nlp_env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# pour le principe, puisque ici pas de gold standard, juste les modèles entre eux\n",
    "df_score = df_score[0:100]\n",
    "print(\n",
    "    classification_report(df_score[\"prediction_long\"], df_score[\"prediction\"], digits=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cacea899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21757606663267043"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_score[\"prediction_long\"], df_score[\"prediction\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e555ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>AUTRE</th>\n",
       "      <th>AUTRE.</th>\n",
       "      <th>REPUBLIQUE</th>\n",
       "      <th>République</th>\n",
       "      <th>République.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_long</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUTRE</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REPUBLIQUE</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction       AUTRE  AUTRE.  REPUBLIQUE  République  République.\n",
       "prediction_long                                                    \n",
       "AUTRE                5       1          23           0            0\n",
       "REPUBLIQUE           3       1          65           1            1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparer les modèles\n",
    "pd.crosstab(df_score[\"prediction_long\"], df_score[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67121355",
   "metadata": {},
   "source": [
    "## Few-shot ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54426f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's build the prompt, with some examples\n",
    "# few_shot_examples = [\n",
    "#         (\"Biden signs executive order on student debt relief\", \"POLITICS\"),\n",
    "#         (\"Amazon launches new AI-powered Alexa features\", \"OTHER\"),\n",
    "#         (\"Congress debates military spending bill\", \"POLITICS\"),\n",
    "#         (\"PSG wins much-anticipated Champions League\", \"OTHER\")\n",
    "#         ]\n",
    "\n",
    "# def get_prompt_few_shots(text: str, examples: list[tuple] = few_shot_examples):\n",
    "#   examples = \"\\n\".join([f\"Classify this headline:\\n{headline}\\nLabel: {label}\\n\\n\" for headline, label in examples])\n",
    "#   return [{\"role\":\"system\",\n",
    "#            \"content\":\"You are a strict news classifier. You must respond with one word only — either 'POLITICS' or 'OTHER'.\" +\n",
    "#            \"\\n Do not explain. Do not output anything else.\"\n",
    "\n",
    "#            },\n",
    "#            {\"role\":\"user\",\"content\": f\"{examples}\\nClassify this headline:\\n\\\"{text}\\\"\\nLabel:\"}\n",
    "#   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d771928",
   "metadata": {},
   "source": [
    "## Refacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspiration GPT pour avoir des checkpoints :\n",
    "# results = []\n",
    "# indexes = []\n",
    "# batch_size = 20\n",
    "\n",
    "# for i in range(0, len(df), batch_size):\n",
    "#     batch = df[\"Texte\"].iloc[i:i+batch_size]\n",
    "#     try:\n",
    "#         r_batch, _ = do_predictions(get_prompt, batch, model=\"llama3.1:70b\")\n",
    "#         results.extend(r_batch)\n",
    "#         indexes.extend(batch.index)\n",
    "\n",
    "#         # Checkpoint every 100 samples\n",
    "#         if i % 100 == 0:\n",
    "#             df.loc[indexes, \"prediction_simple\"] = results\n",
    "#             df.to_csv(\"checkpoint_predictions.csv\", index=False)\n",
    "#             print(f\"Checkpoint sauvegardé à {i}.\")\n",
    "\n",
    "#         time.sleep(1)  # optional pause\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur au batch {i}: {e}\")\n",
    "#         continue\n",
    "\n",
    "# df.loc[indexes, \"prediction_simple\"] = results\n",
    "# df.to_csv(\"final_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf68914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Et même inspiration pour un truc encore plus complet, mais overkill pour l'instant\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# def ensure_dir(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "\n",
    "\n",
    "# def timestamp():\n",
    "#     return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# def do_predictions(prompt_func, texts, model):\n",
    "#     prompt_name = prompt_func.__name__\n",
    "#     results = [prompt_func(text) for text in texts]\n",
    "#     return results, prompt_name\n",
    "\n",
    "\n",
    "# def run_llm_batches(df, text_column, prompt_func, model,\n",
    "#                     column_name=None, batch_size=20, sleep_time=1,\n",
    "#                     checkpoint_every=100, output_dir=\"outputs\"):\n",
    "\n",
    "#     # Setup\n",
    "#     start = time.time()\n",
    "#     all_results = []\n",
    "#     all_indexes = []\n",
    "#     name = prompt_func.__name__ if column_name is None else column_name\n",
    "\n",
    "#     ensure_dir(output_dir)\n",
    "\n",
    "#     print(f\"🚀 Début du traitement avec le prompt '{name}' et le modèle '{model}'\")\n",
    "\n",
    "#     # Boucle par batch\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batch = df[text_column].iloc[i:i + batch_size]\n",
    "#         idx = batch.index\n",
    "\n",
    "#         try:\n",
    "#             batch_start = time.time()\n",
    "#             r_batch, _ = do_predictions(prompt_func, batch, model=model)\n",
    "#             all_results.extend(r_batch)\n",
    "#             all_indexes.extend(idx)\n",
    "#             print(f\"✅ Batch {i}-{i + batch_size} traité en {time.time() - batch_start:.2f}s\")\n",
    "\n",
    "#             # Sauvegarde périodique\n",
    "#             if (i // batch_size) % (checkpoint_every // batch_size) == 0:\n",
    "#                 df.loc[all_indexes, name] = all_results\n",
    "#                 ckpt_file = os.path.join(output_dir, f\"checkpoint_{name}_{timestamp()}.csv\")\n",
    "#                 df.to_csv(ckpt_file, index=False)\n",
    "#                 print(f\"💾 Checkpoint enregistré : {ckpt_file}\")\n",
    "\n",
    "#             time.sleep(sleep_time)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Erreur au batch {i}-{i + batch_size}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     # Résultat final\n",
    "#     df.loc[all_indexes, name] = all_results\n",
    "#     final_file = os.path.join(output_dir, f\"final_{name}_{timestamp()}.csv\")\n",
    "#     df.to_csv(final_file, index=False)\n",
    "\n",
    "#     print(f\"🏁 Traitement terminé en {time.time() - start:.2f}s\")\n",
    "#     print(f\"📝 Résultat final enregistré : {final_file}\")\n",
    "#     return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
