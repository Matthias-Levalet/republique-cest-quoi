{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c6366",
   "metadata": {},
   "source": [
    "# Identifier les textes évoquant le concept de république"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a946c393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215127, 59)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/interim/data_cleaning.csv\", low_memory=False, dtype={\"ID_orateur\": str}\n",
    ")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736da8ed",
   "metadata": {},
   "source": [
    "## INTRODUIRE PRÉ-TRAITEMENT TEXTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30ebc5",
   "metadata": {},
   "source": [
    "Pour simplifier la vie et faciliter aussi possibles perf d'un futur modèle, virer les parenthèses et balises ici pour s'économiser pas mal de choses du côté des noms de groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d67f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyage moche du texte\n",
    "\n",
    "\n",
    "def nettoyer_texte(texte):\n",
    "    if not isinstance(texte, str):\n",
    "        return texte\n",
    "    # Supprimer les balises HTML/XML\n",
    "    texte = re.sub(r\"<[^>]+>\", \"\", texte)\n",
    "    # Supprimer contenu entre parenthèses ou crochets\n",
    "    texte = re.sub(r\"\\([^)]*\\)\", \"\", texte)\n",
    "    # # TODO: aviser si enlève crochets, ou juste […] [...], surtout des troncatures citations\n",
    "    # texte = re.sub(r\"\\[[^\\]]*\\]\", \"\", texte)\n",
    "    # Supprimer les espaces multiples\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
    "    return texte\n",
    "\n",
    "\n",
    "df[\"Texte_clean\"] = df[\"Texte\"].apply(nettoyer_texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53484c90",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d9cdb",
   "metadata": {},
   "source": [
    "Logique de la tentative :\n",
    "- regex\n",
    "- mais exclure certains termes\n",
    "- mais comme les termes exclus peuvent apparaitre aussi avec les termes voulu, éviter de chainer et finir par virer des trucs qu'on aurait voulu (les idées républicaines sont menacées par Les Républicains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46a4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex du champ lexical République (simplifié ici)\n",
    "\n",
    "pattern_lexical = re.compile(\n",
    "    r\"républi\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "# Regex des expressions à exclure\n",
    "# ici garder la casse\n",
    "pattern_excl = re.compile(\n",
    "    r\"\\b[Ll]es Républicains\\b\"\n",
    "    r\"|\\b[Pp]résident de la République\\b\"\n",
    "    r\"|\\b[Pp]résidence de la République\\b\"\n",
    "    r\"\\b[Gg]auche démocrate et républicaine\\b\"\n",
    "    r\"|\\b[Rr]épublique en marche\\b\"\n",
    "    r\"|\\b[Pp]rocureur de la République\\b\"\n",
    "    r\"|\\b[Cc]our(?:s)? de justice de la République\\b\"\n",
    "    r\"|\\b[Aa]dministration générale de la République\\b\"\n",
    "    # r\"|\" + pattern_pays  # ajout des exclusions de pays si existe\n",
    ")\n",
    "\n",
    "\n",
    "def contains_lexical_outside_excl(text):\n",
    "    # Trouve toutes les positions des expressions exclues\n",
    "    excl_positions = [m.span() for m in pattern_excl.finditer(text)]\n",
    "\n",
    "    # Fonction pour vérifier si une position est dans une zone exclue\n",
    "    def in_excl(pos):\n",
    "        for start, end in excl_positions:\n",
    "            if start <= pos < end:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Chercher toutes les occurences du champ lexical\n",
    "    for match in pattern_lexical.finditer(text):\n",
    "        start_pos = match.start()\n",
    "        if not in_excl(start_pos):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83da332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer sur la colonne\n",
    "df[\"repu_match_valide\"] = df[\"Texte_clean\"].apply(contains_lexical_outside_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013aa286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4246, 61)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match = df[df[\"repu_match_valide\"]]\n",
    "df_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match.to_csv(\n",
    "    \"../data/interim/df_repu.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,  # a permis de résoudre le soucis d'écart. Checker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177fc0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4246, 61)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérif écriture/lecture ok\n",
    "df_test = pd.read_csv(\"../data/interim/df_repu.csv\")\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d53fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################\n",
    "# # Pour info si avait bourré sans le span\n",
    "# # (et exclu des choses qui nous intéressent en vrai)\n",
    "# ################\n",
    "\n",
    "# # Version bourrin\n",
    "# # Champ lexical : True si le texte contient un terme du champ lexical\n",
    "# lexical_mask = df[\"Texte_clean\"].str.contains(pattern_lexical)\n",
    "\n",
    "# # Exclusion : True si le texte contient une expression à exclure\n",
    "# excl_mask = df[\"Texte_clean\"].str.contains(pattern_excl)\n",
    "\n",
    "# # On garde ceux qui matchent lexicalement mais ne contiennent pas d'exclusion\n",
    "# df[\"repu_match_simple\"] = lexical_mask & ~excl_mask\n",
    "\n",
    "# df[df[\"repu_match_simple\"]].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f9637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4246, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca982d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_repu = df_match.sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f6944e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_repu.to_csv(\n",
    "    \"../data/interim/test_set_repu.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,  # a permis de résoudre le soucis d'écart. Checker\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
