{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b500467",
   "metadata": {},
   "source": [
    "# Récolte des débats à l'Assemblée Nationale  \n",
    "\n",
    "Ce notebook présente les modalités pour automatiser la collecte des CRI (pour compte-rendus intégraux des débats en séance publique à l'Assemblée Nationale). Pour plus d'informations sur le choix et les différentes options relatives aux compte-rendus des débats en séance publiques à l'Assemblée Nationale française entre 2017 et 2024 (cf .../docs/Données_Débats_AN.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db445f",
   "metadata": {},
   "source": [
    "## Étape 1 : collecte des données [X]\n",
    "\n",
    "### Pour l'option 1 : télécharger les xml\n",
    "\n",
    "Lien d’accès :\n",
    "- https://data.assemblee-nationale.fr/archives-anterieures/archives-15e/debats-en-seance-publique\n",
    "- https://data.assemblee-nationale.fr/archives-16e/debats\n",
    "\n",
    "### Pour l'option 2 : script de scrapping\n",
    "\n",
    "Intérêt d'automatiser la récolte de tous les fichiers disponibles sur cette page \n",
    "- https://echanges.dila.gouv.fr/OPENDATA/Debats/AN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c2f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des liens depuis https://echanges.dila.gouv.fr/OPENDATA/Debats/AN/2018/...\n",
      "162 fichiers trouvés.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # nécessaire de l'installer avec pip la première fois.\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 1/ Je crée un objet correspondant à l'url à partir duquel je souhaite scrapper les fichiers taz (à réaliser en changeant l'année pour chacune des années que je souhaite récupérer)\n",
    "BASE_URL = \"https://echanges.dila.gouv.fr/OPENDATA/Debats/AN/2018/\"\n",
    "\n",
    "# 2/ Je crée un objet pour indiquer le dossier (et son emplacement) où je souhaite déposer les fichiers récupérés \n",
    "destination = \"/Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Raw/DILA/2018\"\n",
    "\n",
    "# 3/ J'écris une fonction pour aller récupérer tous les liens vers les fichiers taz depuis la page html/le lien url donné\n",
    "def get_taz_links(base_url): #je définis par un nom qui correspond (aller récuper les liens taz avec entre parenthèse l'objet crée précédemment)\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") #Beautifulsoup c'est le package que j'ai installé précédement (besoin de le faire qu'une seule fois donc présent ici dans un autre bloc de code) et qui permet le mieux de scrapper à partir d'url (cf le cours de Emilien et Léo sur Python)\n",
    "\n",
    "    taz_links = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and href.endswith(\".taz\"):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            taz_links.append(full_url)\n",
    "\n",
    "    return taz_links\n",
    "# 4/ Je crée une fonction pour télécharger les fichiers depuis l'URL et les mettre dans le dossier défini à l'étape 2\n",
    "def download_file(url, destination):\n",
    "    local_filename = os.path.join(destination, url.split(\"/\")[-1])\n",
    "\n",
    "    if os.path.exists(local_filename):\n",
    "        print(f\"Déjà téléchargé : {local_filename}\")\n",
    "        return\n",
    "\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "def main():\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "    print(f\"Récupération des liens depuis {BASE_URL}...\")\n",
    "    taz_links = get_taz_links(BASE_URL)\n",
    "    print(f\"{len(taz_links)} fichiers trouvés.\")\n",
    "\n",
    "    for link in taz_links:\n",
    "        download_file(link, destination)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# To do : créer une boucle pour le faire sur tous les fichiers de 2011 à 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b213a",
   "metadata": {},
   "source": [
    "## Étape 2 : Décompresser les fichiers taz et répartir entre AAA et CRI (uniquement option 2)\n",
    "\n",
    "2 problèmes rencontrés : 1 celui de la gestion et traitement de fichiers TAZ, et 2 même quand des XML obtenus, à l'exception de 2011-2013, je n'ai pas réussi à récupérer des fichiers de sorties propres. Il manque systèmatiquement les \"retours à la ligne\".\n",
    "\n",
    "\n",
    "Comme écrit précédemment, sur la base de données du gouvernement géré par DILA, on trouve les débats sous formes de fichiers taz (ce sont des fichiers étrange, à la fois décompressé et difficilement ouvrable) comprenant chacun un AAA et un CRI. \n",
    "Ayant galéré à écrire un script qui fasse tout fonctionner, j'ai décomposé : \n",
    "\n",
    "1. Je transforme mes fichiers taz en tar pour avoir un fichier plus facilement exploitable\n",
    "2. Je décompresse mes fichiers tar\n",
    "3. Je déplace les fichiers AAA et CRI dans 2 dossiers séparés (où cette fois tous les CRI sont ensemble pour être mis dans un seul CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8fc0e",
   "metadata": {},
   "source": [
    "### Partie 1 de l'étape 3 (transformer fichiers taz en tar)\n",
    "\n",
    "Le premier script ne fonctionne que pour les fichiers de 2016 à 2024 puisque les fichiers de 2011 à 2015 sont compressés avec l’ancien format UNIX appelé compress qui utilise l’extension .Z. \n",
    "\n",
    "Plus précisément : \n",
    "| Année     | Contenu probable du `.taz`                   | Format interne    |\n",
    "| --------- | -------------------------------------------- | ----------------- |\n",
    "| 2011-2015 | `.Z` compressant un seul `.xml`              | **compress (.Z)** |\n",
    "| 2016-2018 | `.Z` contenant un fichier `.tar` avec `.xml` | **tar.Z**         |\n",
    "| 2019–2021 | `.gz` contenant un `.tar` → `.xml`           | **tar.gz**        |\n",
    "| 2022–2024 | `.taz` ≈ `.xml` brut                         | **non compressé** |\n",
    "\n",
    "==> Essayer de régler ces problèmes avec Léo (multiples tentatives sur le fichier test mais tous des échecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eac8b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction de : AN_2022001.taz\n",
      "Fichier extrait avec succès : AN_2022001.taz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/0c_w194n61ggswq_j8v40mj80000gn/T/ipykernel_1731/3817558419.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=folder_path)\n"
     ]
    }
   ],
   "source": [
    "# Script pour les fichiers 2016-2024\n",
    "# Ici ce que je fais, c'est que je transforme tous mes fichiers taz en fichiers tar (peut-il qu'il existe un moyen plus rapide de les traiter dont j'ai pas connaissance pour l'instant, à creuser)\n",
    "import os # module pour gérer mon ordi\n",
    "import tarfile # module pour gérer les fichiers tar\n",
    "\n",
    "def extract_all_taz_as_tar(folder_path):\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"Dossier introuvable : {folder_path}\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):  \n",
    "        if not filename.endswith(\".taz\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Extraction de : {filename}\")\n",
    "\n",
    "        try:\n",
    "            with tarfile.open(file_path, \"r\") as tar:\n",
    "                tar.extractall(path=folder_path)\n",
    "            print(f\"Fichier extrait avec succès : {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction de {filename} : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dossier = \"/Users/matthiaslevalet/Desktop/Try\"\n",
    "    extract_all_taz_as_tar(dossier)\n",
    "\n",
    "# To do : créer une boucle pour répéter cette opération sur tous les sous-dossiers de 2016 à X (puisque réparti par année). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc8cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Traitement : AN_2022001.taz\n",
      "[ERREUR] uncompress a échoué pour AN_2022001.taz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uncompress: /Users/matthiaslevalet/Desktop/Try/AN_2022001.Z: Inappropriate file type or format\n"
     ]
    }
   ],
   "source": [
    "# Créer un autre script pour gérer le soucis des fichiers de 2011 à 2015 qui sont compressés différemment.  FONCTIONNE DE 2011 À 2013 mais problème pour les deux années suivantes où il n'y a pas de lignes terminator\n",
    "# Il faut vérifier que ce soit le bon script qui a fonctionné, vérifier avec archives de chat\n",
    "import subprocess\n",
    "import shutil\n",
    "import re\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_xml_documents(content):\n",
    "    \"\"\"Extrait les blocs XML complets depuis du contenu concaténé.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        rb'(<\\?xml[^>]*\\?>.*?</[^>]+>)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    return pattern.findall(content)\n",
    "\n",
    "def extract_and_validate_documents(filepath, output_dir, base_name):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "\n",
    "        # Nettoyage des caractères nuls\n",
    "        cleaned_data = raw_data.replace(b'\\x00', b'').strip()\n",
    "\n",
    "        # Extraction des documents XML\n",
    "        xml_blocks = extract_xml_documents(cleaned_data)\n",
    "\n",
    "        if len(xml_blocks) < 2:\n",
    "            print(f\"[ERREUR] Moins de 2 documents XML trouvés dans {filepath.name}\")\n",
    "            return False\n",
    "\n",
    "        xml1, xml2 = xml_blocks[:2]\n",
    "\n",
    "        # Validation syntaxique\n",
    "        for i, xml_bytes in enumerate([xml1, xml2], start=1):\n",
    "            try:\n",
    "                ET.fromstring(xml_bytes.decode('utf-8'))\n",
    "            except ET.ParseError as e:\n",
    "                print(f\"[ERREUR XML] Document {i} mal formé : {e}\")\n",
    "                return False\n",
    "\n",
    "        # Sauvegarde\n",
    "        aaa_path = output_dir / f\"AAA_{base_name}.xml\"\n",
    "        cri_path = output_dir / f\"CRI_{base_name}.xml\"\n",
    "\n",
    "        with open(aaa_path, 'wb') as f1:\n",
    "            f1.write(xml1)\n",
    "        with open(cri_path, 'wb') as f2:\n",
    "            f2.write(xml2)\n",
    "\n",
    "        print(f\"[OK] Extraction réussie : {aaa_path.name}, {cri_path.name}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERREUR] Exception lors de l’extraction de {filepath.name} : {e}\")\n",
    "        return False\n",
    "\n",
    "def decompress_taz_and_extract(source_dir, output_dir):\n",
    "    source_dir = Path(source_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for taz_file in source_dir.rglob(\"*.taz\"):\n",
    "        print(f\"\\n[INFO] Traitement : {taz_file.name}\")\n",
    "\n",
    "        temp_z_file = taz_file.with_suffix(\".Z\")\n",
    "        try:\n",
    "            shutil.copy2(taz_file, temp_z_file)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERREUR] Échec de la copie : {e}\")\n",
    "            continue\n",
    "\n",
    "        # Décompression avec uncompress\n",
    "        try:\n",
    "            subprocess.run([\"uncompress\", str(temp_z_file)], check=True)\n",
    "            decompressed_file = temp_z_file.with_suffix(\"\")\n",
    "            print(f\"[OK] Décompressé : {decompressed_file.name}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"[ERREUR] uncompress a échoué pour {taz_file.name}\")\n",
    "            temp_z_file.unlink(missing_ok=True)\n",
    "            continue\n",
    "\n",
    "        # Extraction et validation\n",
    "        base_name = taz_file.stem\n",
    "        success = extract_and_validate_documents(decompressed_file, output_dir, base_name)\n",
    "\n",
    "        if success:\n",
    "            decompressed_file.unlink(missing_ok=True)  # Supprimer le fichier temporaire\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dossier_source =\"/Users/matthiaslevalet/Desktop/Try\"        # ← À modifier\n",
    "    dossier_sortie = \"/Users/matthiaslevalet/Desktop/Try\"     # ← À modifier\n",
    "    decompress_taz_and_split_clean_xml(dossier_source, dossier_sortie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gérer le problème de 2014 et 2015 \n",
    "    # Problème = absence de \"line terminators\". \n",
    "# Pour les solutions essayées voir notebook 2014-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea52d0",
   "metadata": {},
   "source": [
    "### Partie 2 de l'étape 3 je décompresse les fichiers tar (pour les fichiers de 2016 à 2024, pour 2011-2013 pas besoin car déjà décompressés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f125863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting AN_2022001.tar to /Users/matthiaslevalet/Desktop/Try/AN_2022001...\n",
      "[SUCCESS] Extracted: AN_2022001.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/0c_w194n61ggswq_j8v40mj80000gn/T/ipykernel_1731/2581559503.py:23: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_subdir)\n"
     ]
    }
   ],
   "source": [
    "# Partie 2 : je décompresse les fichiers tar (pour les fichiers de 2016 à 2024, pour 2011-2013 pas besoin car déjà décompressés)\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "def decompress_tar_files(source_dir, target_dir):\n",
    "    source_dir = Path(source_dir)\n",
    "    target_dir = Path(target_dir)\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for tar_path in source_dir.glob(\"*.tar\"):\n",
    "        # Crée un dossier de destination spécifique pour chaque archive\n",
    "        extract_subdir = target_dir / tar_path.stem\n",
    "        if extract_subdir.exists():\n",
    "            print(f\"[INFO] Skipping {tar_path.name}, already extracted.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Extracting {tar_path.name} to {extract_subdir}...\")\n",
    "        extract_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with tarfile.open(tar_path, \"r\") as tar:\n",
    "                tar.extractall(path=extract_subdir)\n",
    "            print(f\"[SUCCESS] Extracted: {tar_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to extract {tar_path.name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_folder = \"/Users/matthiaslevalet/Desktop/Try\"  # à modifier\n",
    "    target_folder = \"/Users/matthiaslevalet/Desktop/Try\"   # à modifier\n",
    "    decompress_tar_files(source_folder, target_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef8b19",
   "metadata": {},
   "source": [
    "### Partie 3 de l'étape 3 : je trie les xml entre compte rendus et annexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe58d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DÉPLACÉ] CRI_AN_20111025_094.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111025_094.xml\n",
      "[DÉPLACÉ] CRI_AN_20111214_122.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111214_122.xml\n",
      "[DÉPLACÉ] CRI_AN_20111123_111.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111123_111.xml\n",
      "[DÉPLACÉ] CRI_AN_20111012_087.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111012_087.xml\n",
      "[DÉPLACÉ] AAA_AN_20111223_128.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111223_128.xml\n",
      "[DÉPLACÉ] AAA_AN_20111208_120.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111208_120.xml\n",
      "[DÉPLACÉ] CRI_AN_20111115_106.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111115_106.xml\n",
      "[DÉPLACÉ] AAA_AN_20111007_086.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111007_086.xml\n",
      "[DÉPLACÉ] AAA_AN_20111110_104.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111110_104.xml\n",
      "[DÉPLACÉ] AAA_AN_20111021_092.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111021_092.xml\n",
      "[DÉPLACÉ] AAA_AN_20111108_102.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111108_102.xml\n",
      "[DÉPLACÉ] CRI_AN_20111029_098.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111029_098.xml\n",
      "[DÉPLACÉ] CRI_AN_20111104_100.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111104_100.xml\n",
      "[DÉPLACÉ] CRI_AN_20111105_101.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111105_101.xml\n",
      "[DÉPLACÉ] AAA_AN_20111109_103.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111109_103.xml\n",
      "[DÉPLACÉ] AAA_AN_20111013_088.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111013_088.xml\n",
      "[DÉPLACÉ] CRI_AN_20111222_127.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111222_127.xml\n",
      "[DÉPLACÉ] AAA_AN_20111111_105.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111111_105.xml\n",
      "[DÉPLACÉ] AAA_AN_20111006_085.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111006_085.xml\n",
      "[DÉPLACÉ] AAA_AN_20111020_091.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111020_091.xml\n",
      "[DÉPLACÉ] CRI_AN_20111118_109.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111118_109.xml\n",
      "[DÉPLACÉ] AAA_AN_20111209_121.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111209_121.xml\n",
      "[DÉPLACÉ] CRI_AN_20111201_116.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111201_116.xml\n",
      "[DÉPLACÉ] AAA_AN_20111028_097.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111028_097.xml\n",
      "[DÉPLACÉ] CRI_AN_20111215_123.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111215_123.xml\n",
      "[DÉPLACÉ] CRI_AN_20111122_110.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111122_110.xml\n",
      "[DÉPLACÉ] CRI_AN_20111216_124.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111216_124.xml\n",
      "[DÉPLACÉ] AAA_AN_20111203_118.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111203_118.xml\n",
      "[DÉPLACÉ] AAA_AN_20111125_113.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111125_113.xml\n",
      "[DÉPLACÉ] AAA_AN_20111022_093.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111022_093.xml\n",
      "[DÉPLACÉ] CRI_AN_20111116_107.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111116_107.xml\n",
      "[DÉPLACÉ] CRI_AN_20111220_125.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111220_125.xml\n",
      "[DÉPLACÉ] AAA_AN_20111018_089.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111018_089.xml\n",
      "[DÉPLACÉ] CRI_AN_20111026_095.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111026_095.xml\n",
      "[DÉPLACÉ] AAA_AN_20111103_099.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111103_099.xml\n",
      "[DÉPLACÉ] CRI_AN_20111019_090.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111019_090.xml\n",
      "[DÉPLACÉ] AAA_AN_20111004_083.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111004_083.xml\n",
      "[DÉPLACÉ] CRI_AN_20111130_115.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111130_115.xml\n",
      "[DÉPLACÉ] CRI_AN_20111202_117.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111202_117.xml\n",
      "[DÉPLACÉ] CRI_AN_20111129_114.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111129_114.xml\n",
      "[DÉPLACÉ] CRI_AN_20111221_126.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111221_126.xml\n",
      "[DÉPLACÉ] AAA_AN_20111124_112.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111124_112.xml\n",
      "[DÉPLACÉ] AAA_AN_20111005_084.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111005_084.xml\n",
      "[DÉPLACÉ] CRI_AN_20111027_096.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111027_096.xml\n",
      "[DÉPLACÉ] CRI_AN_20111207_119.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111207_119.xml\n",
      "[DÉPLACÉ] AAA_AN_20111117_108.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111117_108.xml\n",
      "[DÉPLACÉ] AAA_AN_20111027_096.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111027_096.xml\n",
      "[DÉPLACÉ] AAA_AN_20111207_119.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111207_119.xml\n",
      "[DÉPLACÉ] CRI_AN_20111117_108.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111117_108.xml\n",
      "[DÉPLACÉ] AAA_AN_20111129_114.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111129_114.xml\n",
      "[DÉPLACÉ] AAA_AN_20111221_126.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111221_126.xml\n",
      "[DÉPLACÉ] CRI_AN_20111124_112.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111124_112.xml\n",
      "[DÉPLACÉ] CRI_AN_20111005_084.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111005_084.xml\n",
      "[DÉPLACÉ] CRI_AN_20111004_083.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111004_083.xml\n",
      "[DÉPLACÉ] AAA_AN_20111130_115.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111130_115.xml\n",
      "[DÉPLACÉ] AAA_AN_20111202_117.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111202_117.xml\n",
      "[DÉPLACÉ] AAA_AN_20111019_090.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111019_090.xml\n",
      "[DÉPLACÉ] AAA_AN_20111026_095.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111026_095.xml\n",
      "[DÉPLACÉ] CRI_AN_20111103_099.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111103_099.xml\n",
      "[DÉPLACÉ] AAA_AN_20111116_107.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111116_107.xml\n",
      "[DÉPLACÉ] AAA_AN_20111220_125.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111220_125.xml\n",
      "[DÉPLACÉ] CRI_AN_20111018_089.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111018_089.xml\n",
      "[DÉPLACÉ] CRI_AN_20111125_113.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111125_113.xml\n",
      "[DÉPLACÉ] CRI_AN_20111022_093.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111022_093.xml\n",
      "[DÉPLACÉ] AAA_AN_20111216_124.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111216_124.xml\n",
      "[DÉPLACÉ] CRI_AN_20111203_118.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111203_118.xml\n",
      "[DÉPLACÉ] AAA_AN_20111215_123.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111215_123.xml\n",
      "[DÉPLACÉ] AAA_AN_20111122_110.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111122_110.xml\n",
      "[DÉPLACÉ] CRI_AN_20111020_091.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111020_091.xml\n",
      "[DÉPLACÉ] AAA_AN_20111118_109.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111118_109.xml\n",
      "[DÉPLACÉ] CRI_AN_20111209_121.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111209_121.xml\n",
      "[DÉPLACÉ] AAA_AN_20111201_116.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111201_116.xml\n",
      "[DÉPLACÉ] CRI_AN_20111028_097.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111028_097.xml\n",
      "[DÉPLACÉ] CRI_AN_20111013_088.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111013_088.xml\n",
      "[DÉPLACÉ] CRI_AN_20111109_103.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111109_103.xml\n",
      "[DÉPLACÉ] AAA_AN_20111222_127.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111222_127.xml\n",
      "[DÉPLACÉ] CRI_AN_20111111_105.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111111_105.xml\n",
      "[DÉPLACÉ] CRI_AN_20111006_085.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111006_085.xml\n",
      "[DÉPLACÉ] AAA_AN_20111105_101.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111105_101.xml\n",
      "[DÉPLACÉ] AAA_AN_20111029_098.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111029_098.xml\n",
      "[DÉPLACÉ] AAA_AN_20111104_100.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111104_100.xml\n",
      "[DÉPLACÉ] CRI_AN_20111110_104.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111110_104.xml\n",
      "[DÉPLACÉ] CRI_AN_20111021_092.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111021_092.xml\n",
      "[DÉPLACÉ] CRI_AN_20111108_102.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111108_102.xml\n",
      "[DÉPLACÉ] CRI_AN_20111208_120.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111208_120.xml\n",
      "[DÉPLACÉ] AAA_AN_20111115_106.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111115_106.xml\n",
      "[DÉPLACÉ] CRI_AN_20111007_086.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111007_086.xml\n",
      "[DÉPLACÉ] AAA_AN_20111025_094.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111025_094.xml\n",
      "[DÉPLACÉ] AAA_AN_20111214_122.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111214_122.xml\n",
      "[DÉPLACÉ] AAA_AN_20111123_111.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111123_111.xml\n",
      "[DÉPLACÉ] AAA_AN_20111012_087.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA/AAA_AN_20111012_087.xml\n",
      "[DÉPLACÉ] CRI_AN_20111223_128.xml → /Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI/CRI_AN_20111223_128.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def move_xml_files_by_prefix(source_dir, aaa_dir, cri_dir):\n",
    "    source_dir = Path(source_dir)\n",
    "    aaa_dir = Path(aaa_dir)\n",
    "    cri_dir = Path(cri_dir)\n",
    "\n",
    "    # Recherche dans tous les sous-dossiers\n",
    "    for xml_file in source_dir.rglob(\"*.xml\"):\n",
    "        filename = xml_file.name\n",
    "\n",
    "        if filename.startswith(\"AAA\"):\n",
    "            dest = aaa_dir / filename\n",
    "        elif filename.startswith(\"CRI\"):\n",
    "            dest = cri_dir / filename\n",
    "        else:\n",
    "            print(f\"[IGNORÉ] {filename} ne commence ni par AAA ni par CRI.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            shutil.move(str(xml_file), str(dest))\n",
    "            print(f\"[DÉPLACÉ] {filename} → {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERREUR] Impossible de déplacer {filename} : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_folder = \"/Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Raw/DILA/2011\" \n",
    "    aaa_target = \"/Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/AAA\" \n",
    "    cri_target = \"/Users/matthiaslevalet/Desktop/Projet de recherche/CSS_République/Data/Interim/DILA/CRI\"  \n",
    "\n",
    "    move_xml_files_by_prefix(source_folder, aaa_target, cri_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
